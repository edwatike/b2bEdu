"""Domain Info Parser - –∏–∑–≤–ª–µ–∫–∞–µ—Ç –ò–ù–ù –∏ email —Å –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü.

Multi-strategy approach:
  Tier 1: HTTP probe (httpx) ‚Äî fast, no browser
  Tier 2: API sniff (__NEXT_DATA__, JSON-LD, embedded JSON)
  Tier 3: Playwright browser ‚Äî only when Tier 1+2 fail
"""
import re
import asyncio
import json
import time
import tempfile
import os
from typing import Optional, Dict, List
from urllib.parse import urljoin, urlparse
import logging

import httpx
from playwright.async_api import async_playwright, Browser, Page, TimeoutError as PlaywrightTimeout

# Ensure we can import local modules even when running from backend
import sys
import os
current_dir = os.path.dirname(os.path.abspath(__file__))
if current_dir not in sys.path:
    sys.path.append(current_dir)

from learning_engine import LearningEngine

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class DomainInfoParser:
    """–ü–∞—Ä—Å–µ—Ä –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ò–ù–ù –∏ email —Å –¥–æ–º–µ–Ω–æ–≤."""
    
    def __init__(self, headless: bool = True, timeout: int = 15000):
        """
        Args:
            headless: –ó–∞–ø—É—Å–∫–∞—Ç—å –±—Ä–∞—É–∑–µ—Ä –≤ headless —Ä–µ–∂–∏–º–µ
            timeout: –¢–∞–π–º–∞—É—Ç –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –≤ –º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥–∞—Ö
        """
        self.headless = headless
        self.timeout = timeout
        self.browser: Optional[Browser] = None
        self.playwright = None
        self.learning_engine = LearningEngine()

    def _build_priority_urls(self, domain: str, base_url: str) -> List[str]:
        """Build priority URLs based on learned patterns."""
        if not self.learning_engine:
            return []

        try:
            priority_items = self.learning_engine.get_priority_urls(domain, data_type="both")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –æ–±—É—á–µ–Ω–∏—è: {e}")
            return []

        priority_urls: List[str] = []
        base_netloc = urlparse(base_url).netloc.lower()
        if base_netloc.startswith("www."):
            base_netloc = base_netloc[4:]
        for item in priority_items:
            if not item:
                continue
            item_str = str(item).strip()
            if not item_str:
                continue
            if item_str.startswith("http://") or item_str.startswith("https://"):
                url = item_str
            elif item_str.startswith("/"):
                url = urljoin(base_url, item_str)
            else:
                url = urljoin(base_url, f"/{item_str}")
            cand_netloc = urlparse(url).netloc.lower()
            if cand_netloc.startswith("www."):
                cand_netloc = cand_netloc[4:]
            if cand_netloc == base_netloc or cand_netloc.endswith(f".{base_netloc}"):
                priority_urls.append(url)

        return list(dict.fromkeys(priority_urls))
        
    async def start(self):
        """–ó–∞–ø—É—Å—Ç–∏—Ç—å –±—Ä–∞—É–∑–µ—Ä."""
        logger.info("–ó–∞–ø—É—Å–∫ Playwright...")
        self.playwright = await async_playwright().start()
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—ã—á–Ω—ã–π –∑–∞–ø—É—Å–∫ –±—Ä–∞—É–∑–µ—Ä–∞ –≤–º–µ—Å—Ç–æ COMET CDP
        self.browser = await self.playwright.chromium.launch(headless=True)
        logger.info("‚úÖ –ë—Ä–∞—É–∑–µ—Ä –∑–∞–ø—É—â–µ–Ω (Playwright)")
        
    async def close(self):
        """–ó–∞–∫—Ä—ã—Ç—å –±—Ä–∞—É–∑–µ—Ä."""
        if self.browser:
            await self.browser.close()
        if self.playwright:
            await self.playwright.stop()
        logger.info("‚úÖ –ë—Ä–∞—É–∑–µ—Ä –∑–∞–∫—Ä—ã—Ç")
    
    def extract_inn(self, text: str, html: str = "") -> Optional[str]:
        """–ò–∑–≤–ª–µ—á—å –ò–ù–ù –∏–∑ —Ç–µ–∫—Å—Ç–∞ –∏ HTML —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º–∏."""
        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –ò–ù–ù —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
        inn_patterns = [
            # –ö–†–ò–¢–ò–ß–ù–û: –§–æ—Ä–º–∞—Ç –ò–ù–ù/–ö–ü–ü —Å –∫–æ—Å–æ–π —á–µ—Ä—Ç–æ–π (—Å–∞–º—ã–π —á–∞—Å—Ç—ã–π —Å–ª—É—á–∞–π!)
            r'–ò–ù–ù[/\s]*–ö–ü–ü[:\s]*(\d{10})[/\s]+\d{9}',  # –ò–ù–ù/–ö–ü–ü: 7703412988/772001001
            r'–ò–ù–ù[/\s]*–ö–ü–ü[:\s\n]+(\d{10})[\s/]+\d{9}',  # –ò–ù–ù/–ö–ü–ü 7703412988/772001001
            r'(?:–ò–ù–ù|INN)[/\s]*(?:–ö–ü–ü|KPP)[:\s]*(\d{10})[/\s]+\d{9}',  # INN/KPP: 7703412988/772001001
            
            # –ü—Ä—è–º–æ–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ –ò–ù–ù (—Å —É—á–µ—Ç–æ–º –ø—Ä–æ–±–µ–ª–æ–≤ –∏ –ø–µ—Ä–µ–Ω–æ—Å–æ–≤)
            r'–ò–ù–ù[:\s\n]+(\d{10}|\d{12})',
            r'INN[:\s\n]+(\d{10}|\d{12})',
            r'–∏–Ω–Ω[:\s\n]+(\d{10}|\d{12})',
            
            # –° —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è–º–∏ (–ª—é–±—ã–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ —Ü–∏—Ñ—Ä —Å –ø—Ä–æ–±–µ–ª–∞–º–∏/–¥–µ—Ñ–∏—Å–∞–º–∏)
            r'–ò–ù–ù[:\s\n]+(\d{4}[\s\-\n]?\d{6})',  # –ò–ù–ù: 1234 567890
            r'–ò–ù–ù[:\s\n]+(\d{4}[\s\-\n]?\d{4}[\s\-\n]?\d{4})',  # –ò–ù–ù: 1234 5678 9012
            r'–ò–ù–ù[:\s\n]+(\d{2}[\s\-]\d{3}[\s\-]\d{3}[\s\-]\d{2})',  # –ò–ù–ù: 78 393 394 21
            r'–ò–ù–ù[:\s\n]+(\d[\s\-]?\d[\s\-]?\d[\s\-]?\d[\s\-]?\d[\s\-]?\d[\s\-]?\d[\s\-]?\d[\s\-]?\d[\s\-]?\d)',  # –ò–ù–ù —Å –ª—é–±—ã–º–∏ –ø—Ä–æ–±–µ–ª–∞–º–∏ –º–µ–∂–¥—É —Ü–∏—Ñ—Ä–∞–º–∏
            r'INN[:\s\n]+(\d[\s\-]?\d[\s\-]?\d[\s\-]?\d[\s\-]?\d[\s\-]?\d[\s\-]?\d[\s\-]?\d[\s\-]?\d[\s\-]?\d)',  # INN —Å –ø—Ä–æ–±–µ–ª–∞–º–∏
            
            # –í —Ç–∞–±–ª–∏—Ü–∞—Ö/—Ä–µ–∫–≤–∏–∑–∏—Ç–∞—Ö
            r'(?:—Ä–µ–∫–≤–∏–∑–∏—Ç|requisite|details|—é—Ä–∏–¥–∏—á–µ—Å–∫).*?–ò–ù–ù[:\s\n]*(\d{10}|\d{12})',
            r'(?:—Ä–µ–∫–≤–∏–∑–∏—Ç|requisite|details|legal).*?INN[:\s\n]*(\d{10}|\d{12})',
            
            # –†—è–¥–æ–º —Å –û–ì–†–ù/–ö–ü–ü
            r'(?:–û–ì–†–ù|OGRN)[:\s\n]+\d+.*?–ò–ù–ù[:\s\n]*(\d{10}|\d{12})',
            r'(?:–ö–ü–ü|KPP)[:\s\n]+\d+.*?–ò–ù–ù[:\s\n]*(\d{10}|\d{12})',
            
            # –í –∫–æ–Ω—Ç–∞–∫—Ç–∞—Ö/–æ –∫–æ–º–ø–∞–Ω–∏–∏
            r'(?:–æ –∫–æ–º–ø–∞–Ω–∏–∏|about|–∫–æ–Ω—Ç–∞–∫—Ç|contact|company).*?–ò–ù–ù[:\s\n]*(\d{10}|\d{12})',
            
            # –í —Ñ—É—Ç–µ—Ä–µ
            r'(?:footer|–ø–æ–¥–≤–∞–ª).*?–ò–ù–ù[:\s\n]*(\d{10}|\d{12})',
        ]
        
        # –ò—â–µ–º —Å —è–≤–Ω—ã–º —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ–º –ò–ù–ù –≤ —Ç–µ–∫—Å—Ç–µ
        for pattern in inn_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE | re.DOTALL)
            for match in matches:
                # –£–±–∏—Ä–∞–µ–º –ø—Ä–æ–±–µ–ª—ã, –¥–µ—Ñ–∏—Å—ã –∏ –ø–µ—Ä–µ–Ω–æ—Å—ã
                clean_match = re.sub(r'[\s\-\n]', '', match)
                if len(clean_match) in [10, 12]:
                    logger.info(f"Found INN with pattern in text: {clean_match}")
                    return clean_match
        
        # –ò—â–µ–º –≤ HTML (–µ—Å–ª–∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω)
        if html:
            # –ü–æ–∏—Å–∫ –≤ meta-—Ç–µ–≥–∞—Ö
            meta_patterns = [
                r'<meta[^>]*name=["\']inn["\'][^>]*content=["\'](\d{10}|\d{12})["\']',
                r'<meta[^>]*property=["\']inn["\'][^>]*content=["\'](\d{10}|\d{12})["\']',
                r'<meta[^>]*content=["\'](\d{10}|\d{12})["\'][^>]*name=["\']inn["\']',
            ]
            
            for pattern in meta_patterns:
                matches = re.findall(pattern, html, re.IGNORECASE)
                if matches:
                    logger.info(f"Found INN in meta tag: {matches[0]}")
                    return matches[0]
            
            # –ü–æ–∏—Å–∫ –≤ data-–∞—Ç—Ä–∏–±—É—Ç–∞—Ö
            data_patterns = [
                r'data-inn=["\'](\d{10}|\d{12})["\']',
                r'data-company-inn=["\'](\d{10}|\d{12})["\']',
            ]
            
            for pattern in data_patterns:
                matches = re.findall(pattern, html, re.IGNORECASE)
                if matches:
                    logger.info(f"Found INN in data attribute: {matches[0]}")
                    return matches[0]
            
            # –ü–æ–∏—Å–∫ –≤ HTML —Å —è–≤–Ω—ã–º —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ–º –ò–ù–ù
            for pattern in inn_patterns:
                matches = re.findall(pattern, html, re.IGNORECASE | re.DOTALL)
                for match in matches:
                    clean_match = re.sub(r'[\s\-\n]', '', match)
                    if len(clean_match) in [10, 12]:
                        logger.info(f"Found INN with pattern in HTML: {clean_match}")
                        return clean_match
            
            # –ü–æ–∏—Å–∫ –≤ JavaScript-–∫–æ–Ω—Ç–µ–Ω—Ç–µ (–ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ, –æ–±—ä–µ–∫—Ç—ã, JSON)
            js_patterns = [
                r'["\']inn["\']\s*:\s*["\']?(\d{10}|\d{12})["\']?',  # "inn": "7820067929"
                r'inn\s*=\s*["\']?(\d{10}|\d{12})["\']?',  # inn = "7820067929"
                r'companyInn["\']?\s*:\s*["\']?(\d{10}|\d{12})["\']?',  # companyInn: "7820067929"
                r'data\.inn\s*=\s*["\']?(\d{10}|\d{12})["\']?',  # data.inn = "7820067929"
                r'"tax_id"\s*:\s*"(\d{10}|\d{12})"',  # "tax_id": "7820067929"
                r'"company_id"\s*:\s*"(\d{10}|\d{12})"',  # "company_id": "7820067929"
                r'"ogrn"\s*:\s*"(\d{13})"[^}]*"inn"\s*:\s*"(\d{10}|\d{12})"',  # –û–ì–†–ù + –ò–ù–ù –≤ JSON
                r'"kpp"\s*:\s*"\d{9}"[^}]*"inn"\s*:\s*"(\d{10}|\d{12})"',  # –ö–ü–ü + –ò–ù–ù –≤ JSON
                r'–ò–ù–ù\s*[:\=]\s*["\']?(\d{10}|\d{12})["\']?',  # –ò–ù–ù: "7820067929"
                r'–ò–ù–ù\s*[:\=]\s*(\d{10}|\d{12})',  # –ò–ù–ù: 7820067929
            ]
            
            for pattern in js_patterns:
                matches = re.findall(pattern, html, re.IGNORECASE)
                if matches:
                    logger.info(f"Found INN in JavaScript content: {matches[0]}")
                    return matches[0]
            
            # –£–ë–†–ê–õ: –ê–ì–†–ï–°–°–ò–í–ù–´–ô –ü–û–ò–°–ö - –∏—Å–∫–∞–ª –ª—é–±—ã–µ 10/12-–∑–Ω–∞—á–Ω—ã–µ —á–∏—Å–ª–∞ –≤ HTML —Ä—è–¥–æ–º —Å–æ —Å–ª–æ–≤–∞–º–∏ –ò–ù–ù/INN
# –£–ë–†–ê–õ: context_patterns - –∏—Å–∫–∞–ª —á–∏—Å–ª–∞ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ —Ä–µ–∫–≤–∏–∑–∏—Ç–æ–≤ –±–µ–∑ —è–≤–Ω–æ–≥–æ "–ò–ù–ù"
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —Å —è–≤–Ω—ã–º —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ–º, –∏—â–µ–º 10 –∏–ª–∏ 12 —Ü–∏—Ñ—Ä –ø–æ–¥—Ä—è–¥
        # –Ω–æ —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –æ–Ω–∏ –æ–∫—Ä—É–∂–µ–Ω—ã –ø—Ä–æ–±–µ–ª–∞–º–∏ –∏–ª–∏ –∑–Ω–∞–∫–∞–º–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –ò–õ–ò —Ä—è–¥–æ–º —Å –ò–ù–ù
        # –ë–û–õ–ï–ï –°–¢–†–û–ì–ò–ô –ü–û–î–•–û–î: –Ω–µ –±–µ—Ä–µ–º –ø—Ä–æ—Å—Ç–æ —á–∏—Å–ª–∞ –∏–∑ HTML –±–µ–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        general_pattern = r'(?<!\d)(\d{10}|\d{12})(?!\d)'
        matches = re.findall(general_pattern, text)
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º: –∏—Å–∫–ª—é—á–∞–µ–º —Ç–µ–ª–µ—Ñ–æ–Ω—ã –∏ –¥—Ä—É–≥–∏–µ —á–∏—Å–ª–∞
        for match in matches:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –Ω–µ —Ç–µ–ª–µ—Ñ–æ–Ω (–Ω–µ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å 7, 8, 9)
            if len(match) == 10 and not match.startswith(('7', '8', '9')):
                # –î–û–ü. –ü–†–û–í–ï–†–ö–ê: –∏—â–µ–º "–ò–ù–ù" —Ä—è–¥–æ–º —Å —ç—Ç–∏–º —á–∏—Å–ª–æ–º –≤ —Ç–µ–∫—Å—Ç–µ
                inn_context = re.search(r'.{0,30}' + re.escape(match) + '.{0,30}', text, re.IGNORECASE)
                if inn_context and ('–ò–ù–ù' in inn_context.group() or 'INN' in inn_context.group()):
                    logger.info(f"Found INN with context in text: {match}")
                    return match
            elif len(match) == 12:
                # –î–ª—è 12-–∑–Ω–∞—á–Ω—ã—Ö –ò–ù–ù (–ò–ü) –ø—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –Ω–µ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å 79 (—Ç–µ–ª–µ—Ñ–æ–Ω)
                if not match.startswith('79'):
                    # –î–û–ü. –ü–†–û–í–ï–†–ö–ê: –∏—â–µ–º "–ò–ù–ù" —Ä—è–¥–æ–º —Å —ç—Ç–∏–º —á–∏—Å–ª–æ–º –≤ —Ç–µ–∫—Å—Ç–µ
                    inn_context = re.search(r'.{0,30}' + re.escape(match) + '.{0,30}', text, re.IGNORECASE)
                    if inn_context and ('–ò–ù–ù' in inn_context.group() or 'INN' in inn_context.group()):
                        logger.info(f"Found INN with context in text: {match}")
                        return match
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –ø–æ–∏—Å–∫: –∏—â–µ–º 10-–∑–Ω–∞—á–Ω—ã–µ —á–∏—Å–ª–∞ —Ä—è–¥–æ–º —Å 13-–∑–Ω–∞—á–Ω—ã–º–∏ (–û–ì–†–ù) - –¢–û–õ–¨–ö–û —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –ò–ù–ù
        # –£–ë–†–ê–õ: ogrn_inn_pattern - –∏—Å–∫–∞–ª –ª—é–±—ã–µ —á–∏—Å–ª–∞ —Ä—è–¥–æ–º —Å –û–ì–†–ù
        
        # –ü–æ–∏—Å–∫ –≤ HTML —Å –≤–æ–∑–º–æ–∂–Ω–æ–π –ø—Ä–æ–±–ª–µ–º–æ–π –∫–æ–¥–∏—Ä–æ–≤–∫–∏
        if html:
            # –ò—â–µ–º –ò–ù–ù —Ä—è–¥–æ–º —Å–æ —Å–ª–æ–≤–æ–º –ò–ù–ù (–¥–∞–∂–µ –µ—Å–ª–∏ –∫–∏—Ä–∏–ª–ª–∏—Ü–∞ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∞)
            # –ò—â–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã: –ò–ù–ù + 10 —Ü–∏—Ñ—Ä –ò–õ–ò 10 —Ü–∏—Ñ—Ä + –ò–ù–ù
            inn_context_patterns = [
                r'(?:\xd0\x98\xd0\x9d\xd0\x9d|\xd0\x98\xd0\xbd\xd0\xbd|\xd0\xb8\xd0\xbd\xd0\xbd|\xd0\x98\xd0\xbd\xd0\xbd|\xd0\x98\xd0\x9d\xd0\x9d)[^\d]{0,20}(\d{10})',  # –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ "–ò–ù–ù"
                r'(\d{10})[^\d]{0,20}(?:\xd0\x98\xd0\x9d\xd0\x9d|\xd0\x98\xd0\xbd\xd0\xbd|\xd0\xb8\xd0\xbd\xd0\xbd|\xd0\x98\xd0\xbd\xd0\xbd|\xd0\x98\xd0\x9d\xd0\x9d)',  # –ß–∏—Å–ª–æ –ø–µ—Ä–µ–¥ "–ò–ù–ù"
                r'(?:\xd0\x9a\xd0\x9a\xd0\x9f|\xd0\xba\xd0\xba\xd0\xbf)[^\d]{0,20}\d{9}[^\d]{0,20}(\d{10})',  # –ö–ü–ü + –ò–ù–ù
                r'(\d{10})[^\d]{0,20}\d{9}[^\d]{0,20}(?:\xd0\x9a\xd0\x9a\xd0\x9f|\xd0\xba\xd0\xba\xd0\xbf)',  # –ò–ù–ù + –ö–ü–ü
            ]
            
            for pattern in inn_context_patterns:
                matches = re.findall(pattern, html, re.IGNORECASE)
                for match in matches:
                    if isinstance(match, tuple):
                        match = match[0] if match[0] else match[1]
                    if len(match) == 10 and not match.startswith(('7', '8', '9')):
                        logger.info(f"Found INN with context in HTML: {match}")
                        return match
            
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º, –ù–ï –∏—â–µ–º –ª—é–±—ã–µ 10-–∑–Ω–∞—á–Ω—ã–µ —á–∏—Å–ª–∞ –≤ HTML
            # –≠—Ç–æ –∏—Å–∫–ª—é—á–∞–µ—Ç ID —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –∏ –¥—Ä—É–≥–∏–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —á–∏—Å–ª–∞
            # –ò–ù–ù –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –Ω–∞–π–¥–µ–Ω —Ç–æ–ª—å–∫–æ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º "–ò–ù–ù" –∏–ª–∏ –Ω–∞ –∫–æ–Ω—Ç–∞–∫—Ç–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü–∞—Ö
            
            # –ü–æ–∏—Å–∫ –ò–ù–ù –≤ HTML-—Ç–∞–±–ª–∏—Ü–∞—Ö: <th>–ò–ù–ù</th><td>1234567890</td> –∏–ª–∏ <td>–ò–ù–ù</td><td>1234567890</td>
            table_inn_patterns = [
                # th/td —Å —Ç–µ–∫—Å—Ç–æ–º –ò–ù–ù, –∑–∞—Ç–µ–º td —Å —á–∏—Å–ª–æ–º (—Å –≤–æ–∑–º–æ–∂–Ω—ã–º–∏ –ø—Ä–æ–±–µ–ª–∞–º–∏)
                r'<t[hd][^>]*>\s*–ò–ù–ù\s*</t[hd]>\s*<td[^>]*>\s*([\d\s\-]{10,16})\s*</td>',
                r'<t[hd][^>]*>[^<]*–ò–ù–ù[^<]*</t[hd]>\s*<td[^>]*>\s*([\d\s\-]{10,16})\s*</td>',
                # –í–∞—Ä–∏–∞–Ω—Ç —Å rowheader
                r'rowheader[^>]*>\s*–ò–ù–ù\s*</[^>]+>\s*<[^>]*>\s*([\d\s\-]{10,16})\s*<',
                # –ò–ù–ù –≤ –æ–¥–Ω–æ–π —è—á–µ–π–∫–µ: "–ò–ù–ù: 1234567890" –∏–ª–∏ "–ò–ù–ù 1234567890"
                r'<td[^>]*>[^<]*–ò–ù–ù[:\s]+([\d\s\-]{10,16})[^<]*</td>',
                # dt/dd —Ñ–æ—Ä–º–∞—Ç
                r'<dt[^>]*>[^<]*–ò–ù–ù[^<]*</dt>\s*<dd[^>]*>\s*([\d\s\-]{10,16})\s*</dd>',
            ]
            for pattern in table_inn_patterns:
                matches = re.findall(pattern, html, re.IGNORECASE | re.DOTALL)
                for match in matches:
                    clean = re.sub(r'[\s\-]', '', match)
                    if len(clean) in [10, 12] and not clean.startswith(('7', '8', '9')):
                        logger.info(f"Found INN in HTML table: {clean}")
                        return clean
                    elif len(clean) in [10, 12]:
                        # –î–∞–∂–µ –µ—Å–ª–∏ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å 7/8/9 ‚Äî –≤ —Ç–∞–±–ª–∏—Ü–µ —Å –∑–∞–≥–æ–ª–æ–≤–∫–æ–º –ò–ù–ù —ç—Ç–æ —Ç–æ—á–Ω–æ –ò–ù–ù
                        logger.info(f"Found INN in HTML table (starts with 7/8/9): {clean}")
                        return clean
        
        # –ü–æ—Å–ª–µ–¥–Ω–∏–π —à–∞–Ω—Å: –∏—â–µ–º 10 —Ü–∏—Ñ—Ä (—Å –ø—Ä–æ–±–µ–ª–∞–º–∏) —Ä—è–¥–æ–º —Å –ö–ü–ü/–û–ì–†–ù (–±–µ–∑ —è–≤–Ω–æ–≥–æ "–ò–ù–ù")
        if text:
            kpp_inn_patterns = [
                r'–ö–ü–ü[:\s]+\d{9}[\s,;/]+([\d\s\-]{10,16})',  # –ö–ü–ü: 123456789 / 1234567890
                r'([\d\s\-]{10,16})[\s,;/]+–ö–ü–ü[:\s]+\d{9}',  # 1234567890 / –ö–ü–ü: 123456789
                r'–û–ì–†–ù[:\s]+\d{13}[\s,;/]+([\d\s\-]{10,16})',  # –û–ì–†–ù + —á–∏—Å–ª–æ
            ]
            for pattern in kpp_inn_patterns:
                matches = re.findall(pattern, text, re.IGNORECASE)
                for match in matches:
                    clean = re.sub(r'[\s\-]', '', match)
                    if len(clean) in [10, 12]:
                        logger.info(f"Found INN near KPP/OGRN: {clean}")
                        return clean
        
        logger.info("No INN found in text")
        return None
    
    def extract_emails(self, text: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á—å email –∞–¥—Ä–µ—Å–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞."""
        pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
        emails = re.findall(pattern, text)
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º: –∏—Å–∫–ª—é—á–∞–µ–º –æ–±—â–∏–µ email-–∞–¥—Ä–µ—Å–∞ —Ç–∏–ø–∞ example@example.com
        filtered = []
        exclude_patterns = ['example', 'test', 'domain', 'email', 'yoursite', 'yourdomain']
        
        for email in emails:
            email_lower = email.lower()
            if not any(pattern in email_lower for pattern in exclude_patterns):
                filtered.append(email)
        
        return list(set(filtered))  # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
    
    def extract_emails_from_html(self, html: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á—å email –∞–¥—Ä–µ—Å–∞ –∏–∑ HTML (–≤–∫–ª—é—á–∞—è mailto)."""
        if not html:
            return []

        mailto_pattern = r'mailto:([^"\'\s>]+)'
        emails = re.findall(mailto_pattern, html, re.IGNORECASE)
        cleaned = [email.split("?")[0] for email in emails]
        return list(set(self.extract_emails(" ".join(cleaned))))

    def _is_pdf_url(self, url: str) -> bool:
        """Check if URL points to a PDF file."""
        parsed = urlparse(url)
        return parsed.path.lower().endswith('.pdf')

    async def download_and_parse_pdf(self, url: str) -> Dict:
        """Download PDF and extract INN/email from it.
        
        Returns:
            Dict with keys: text, inn, emails, error
        """
        result = {"text": "", "inn": None, "emails": [], "error": None}
        tmp_path = None
        try:
            async with httpx.AsyncClient(timeout=15.0, follow_redirects=True) as client:
                resp = await client.get(url)
                resp.raise_for_status()
                content_type = resp.headers.get("content-type", "")
                if len(resp.content) > 10_000_000:  # 10MB limit
                    result["error"] = "PDF —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π (>10MB)"
                    return result

            # Save to temp file
            with tempfile.NamedTemporaryFile(suffix=".pdf", delete=False) as f:
                f.write(resp.content)
                tmp_path = f.name

            # Extract text with PyMuPDF (fitz)
            try:
                import fitz  # PyMuPDF
                doc = fitz.open(tmp_path)
                pages_text = []
                for page_num in range(min(doc.page_count, 10)):  # max 10 pages
                    page = doc[page_num]
                    pages_text.append(page.get_text())
                doc.close()
                full_text = "\n".join(pages_text)
                result["text"] = full_text
            except Exception as e:
                # Fallback to PyPDF2
                try:
                    from PyPDF2 import PdfReader
                    reader = PdfReader(tmp_path)
                    pages_text = []
                    for page_num in range(min(len(reader.pages), 10)):
                        pages_text.append(reader.pages[page_num].extract_text() or "")
                    full_text = "\n".join(pages_text)
                    result["text"] = full_text
                except Exception as e2:
                    result["error"] = f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å PDF: {e2}"
                    return result

            # Extract INN and emails from PDF text
            if result["text"]:
                result["inn"] = self.extract_inn(result["text"])
                result["emails"] = self.extract_emails(result["text"])
                if result["inn"]:
                    logger.info(f"  ‚úÖ –ò–ù–ù –Ω–∞–π–¥–µ–Ω –≤ PDF: {result['inn']}")
                if result["emails"]:
                    logger.info(f"  ‚úÖ Email –Ω–∞–π–¥–µ–Ω –≤ PDF: {result['emails']}")

        except httpx.HTTPStatusError as e:
            result["error"] = f"HTTP {e.response.status_code}"
        except Exception as e:
            result["error"] = str(e)[:200]
        finally:
            if tmp_path and os.path.exists(tmp_path):
                try:
                    os.unlink(tmp_path)
                except Exception:
                    pass
        return result

    async def goto_with_fallback(self, page: Page, url: str) -> None:
        """–û—Ç–∫—Ä—ã—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É, –ø—Ä–∏ –æ—à–∏–±–∫–µ HTTPS –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å HTTP."""
        try:
            await page.goto(url, wait_until='domcontentloaded', timeout=self.timeout)
            return
        except Exception as e:
            err_str = str(e).lower()
            # If it's a download (PDF), don't retry with HTTP
            if 'download' in err_str:
                raise
            if url.startswith("https://"):
                fallback_url = "http://" + url[len("https://") :]
                logger.warning(f"  ‚ö†Ô∏è HTTPS –Ω–µ —É–¥–∞–ª–æ—Å—å, –ø—Ä–æ–±—É–µ–º HTTP: {fallback_url}")
                await page.goto(fallback_url, wait_until='domcontentloaded', timeout=self.timeout)
                return
            raise e
    
    async def get_page_text(self, page: Page) -> str:
        """–ü–æ–ª—É—á–∏—Ç—å –≤–µ—Å—å —Ç–µ–∫—Å—Ç —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã, –≤–∫–ª—é—á–∞—è —Ç–∞–±–ª–∏—Ü—ã –∏ —Å–∫—Ä—ã—Ç—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã."""
        try:
            text = await page.evaluate('''() => {
                let result = document.body.innerText || '';
                
                // –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –∏–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ —Ç–∞–±–ª–∏—Ü —Å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è–º–∏
                const tables = document.querySelectorAll('table');
                for (const table of tables) {
                    const rows = table.querySelectorAll('tr');
                    for (const row of rows) {
                        const cells = row.querySelectorAll('th, td');
                        const cellTexts = Array.from(cells).map(c => c.innerText.trim()).filter(t => t);
                        if (cellTexts.length > 0) {
                            result += '\\n' + cellTexts.join(': ');
                        }
                    }
                }
                
                // –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ dt/dd —Å–ø–∏—Å–∫–æ–≤
                const dts = document.querySelectorAll('dt');
                for (const dt of dts) {
                    const dd = dt.nextElementSibling;
                    if (dd && dd.tagName === 'DD') {
                        result += '\\n' + dt.innerText.trim() + ': ' + dd.innerText.trim();
                    }
                }
                
                return result;
            }''')
            return text
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {e}")
            return ""
    
    async def find_contact_pages(self, page: Page, base_url: str) -> List[str]:
        """–ù–∞–π—Ç–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –∫–æ–Ω—Ç–∞–∫—Ç–∞–º–∏."""
        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü —Å —Ä–µ–∫–≤–∏–∑–∏—Ç–∞–º–∏
        contact_keywords = [
            '–∫–æ–Ω—Ç–∞–∫—Ç', 'contact', '–æ –∫–æ–º–ø–∞–Ω–∏–∏', '–∫–æ–º–ø–∞–Ω–∏', 'about', 
            '—Ä–µ–∫–≤–∏–∑–∏—Ç', '—Ä–µ–∫–≤–∏–∑–∏—Ç—ã', 'requisites',
            'politics', 'company', '—é—Ä–∏–¥–∏—á–µ—Å–∫', 'legal', 'details', '–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è',
            '—é—Ä. –ª–∏—Ü', '—é—Ä–ª–∏—Ü', '–æ–ø—Ç–æ–≤', '–ø–æ—Å—Ç–∞–≤—â–∏–∫', '–ø–∞—Ä—Ç–Ω–µ—Ä',
            '—Å–æ—Ç—Ä—É–¥–Ω–∏—á–µ—Å—Ç–≤', '–∫–∞—Ä—Ç–æ—á–∫–∞ –ø—Ä–µ–¥–ø—Ä–∏—è—Ç–∏—è', '—Å–≤–µ–¥–µ–Ω–∏—è',
        ]
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –≤ URL
        url_keywords = [
            'contact', 'about', 'requisites', 'requisite', 'requizit', 'requisiti',
            'politics', 'company', 'legal', 'details', 'o-kompanii', 'kompanii', 
            'rekvizit', 'rekvizity', 'kontakt', 'kontakty', 'ur-lic', 'yurlits',
            'opt', 'partner', 'sotrudnich', 'kartochka', 'svedeniya',
            'about-us', 'about_us', 'info',
        ]
        contact_urls = []
        
        try:
            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
            links = await page.evaluate('''() => {
                return Array.from(document.querySelectorAll('a[href]')).map(a => ({
                    href: a.href,
                    text: a.innerText.toLowerCase()
                }));
            }''')
            
            for link in links:
                href = link['href']
                text = link['text']
                href_lower = href.lower()
                text_lower = text.lower()
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–µ–∫—Å—Ç —Å—Å—ã–ª–∫–∏ –ò–õ–ò URL (—á–∞—Å—Ç–∏—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ)
                text_match = any(keyword in text_lower for keyword in contact_keywords)
                url_match = any(keyword in href_lower for keyword in url_keywords)
                
                if text_match or url_match:
                    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –∞–±—Å–æ–ª—é—Ç–Ω—ã–π URL
                    full_url = urljoin(base_url, href)
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ —Ç–æ—Ç –∂–µ –¥–æ–º–µ–Ω
                    if urlparse(full_url).netloc == urlparse(base_url).netloc:
                        contact_urls.append(full_url)
            
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –∫–æ–Ω—Ç–∞–∫—Ç–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü: {e}")
        
        return list(set(contact_urls))[:5]  # –ú–∞–∫—Å–∏–º—É–º 5 —Å—Ç—Ä–∞–Ω–∏—Ü
    
    # ‚îÄ‚îÄ‚îÄ HTTP-first Strategy (Tier 1 & 2) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    _HTTP_HEADERS = {
        "User-Agent": (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/120.0.0.0 Safari/537.36"
        ),
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7",
    }

    def _detect_js_required(self, html: str, status_code: int = 200, headers: dict = None) -> bool:
        """–û–ø—Ä–µ–¥–µ–ª–∏—Ç—å, —Ç—Ä–µ–±—É–µ—Ç—Å—è –ª–∏ JS-—Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥ –¥–ª—è –¥–∞–Ω–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã."""
        if not html or len(html.strip()) < 500:
            return True
        headers = headers or {}
        hdr_str = str(headers).lower()
        html_lower = html.lower()
        if 'cf-ray' in hdr_str or 'cloudflare' in hdr_str:
            if status_code in (403, 503) or 'challenge' in html_lower:
                return True
        body_text = re.sub(r'<script[^>]*>.*?</script>', '', html, flags=re.DOTALL | re.IGNORECASE)
        body_text = re.sub(r'<style[^>]*>.*?</style>', '', body_text, flags=re.DOTALL | re.IGNORECASE)
        body_text = re.sub(r'<[^>]+>', '', body_text).strip()
        spa_markers = ['<div id="app"></div>', '<div id="root"></div>',
                       '<div id="__next"></div>', '<div id="__nuxt">']
        if len(body_text) < 200 and any(m in html_lower for m in spa_markers):
            return True
        return False

    def _sniff_embedded_data(self, html: str) -> Dict:
        """Tier 2: –ò–∑–≤–ª–µ—á—å –ò–ù–ù/email –∏–∑ –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö (__NEXT_DATA__, JSON-LD, etc)."""
        result: Dict = {"inn": None, "emails": [], "source": None}
        if not html:
            return result

        # 1. __NEXT_DATA__
        next_match = re.search(
            r'<script\s+id="__NEXT_DATA__"[^>]*>(.*?)</script>', html, re.DOTALL
        )
        if next_match:
            try:
                data = json.loads(next_match.group(1))
                text_dump = json.dumps(data, ensure_ascii=False)
                inn = self.extract_inn(text_dump)
                emails = self.extract_emails(text_dump)
                if inn:
                    result["inn"] = inn
                    result["source"] = "__NEXT_DATA__"
                if emails:
                    result["emails"] = emails
                    result["source"] = result["source"] or "__NEXT_DATA__"
            except Exception:
                pass

        # 2. __NUXT__
        if not result["inn"]:
            nuxt_match = re.search(
                r'window\.__NUXT__\s*=\s*(\{.*?\});?\s*</script>', html, re.DOTALL
            )
            if nuxt_match:
                try:
                    text_dump = nuxt_match.group(1)
                    inn = self.extract_inn(text_dump)
                    emails = self.extract_emails(text_dump)
                    if inn:
                        result["inn"] = inn
                        result["source"] = "__NUXT__"
                    if emails and not result["emails"]:
                        result["emails"] = emails
                except Exception:
                    pass

        # 3. JSON-LD (schema.org)
        if not result["inn"]:
            ld_matches = re.findall(
                r'<script\s+type="application/ld\+json"[^>]*>(.*?)</script>',
                html, re.DOTALL,
            )
            for ld_text in ld_matches:
                if result["inn"]:
                    break
                try:
                    ld_data = json.loads(ld_text)
                    text_dump = json.dumps(ld_data, ensure_ascii=False)
                    inn = self.extract_inn(text_dump)
                    emails = self.extract_emails(text_dump)
                    if inn:
                        result["inn"] = inn
                        result["source"] = "json-ld"
                    if emails and not result["emails"]:
                        result["emails"] = emails
                except Exception:
                    pass

        return result

    def _extract_contact_links_from_html(self, html: str, base_url: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á—å —Å—Å—ã–ª–∫–∏ –Ω–∞ –∫–æ–Ω—Ç–∞–∫—Ç–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–∑ raw HTML (–±–µ–∑ Playwright)."""
        contact_urls: List[str] = []
        link_pattern = r'<a[^>]+href=["\']([^"\']+)["\'][^>]*>(.*?)</a>'
        links = re.findall(link_pattern, html, re.IGNORECASE | re.DOTALL)

        contact_kw = [
            '–∫–æ–Ω—Ç–∞–∫—Ç', 'contact', '–æ –∫–æ–º–ø–∞–Ω–∏–∏', 'about', '—Ä–µ–∫–≤–∏–∑–∏—Ç',
            'requisit', 'company', '—é—Ä–∏–¥–∏—á–µ—Å–∫', 'legal', 'details',
            'rekvizit', 'kontakt', 'info',
        ]
        url_kw = [
            'contact', 'about', 'requisit', 'company', 'legal',
            'rekvizit', 'kontakt', 'o-kompanii', 'info', 'details',
        ]
        base_netloc = urlparse(base_url).netloc

        for href, text in links:
            text_clean = re.sub(r'<[^>]+>', '', text).strip().lower()
            href_lower = href.lower()
            if any(kw in text_clean for kw in contact_kw) or \
               any(kw in href_lower for kw in url_kw):
                full_url = urljoin(base_url, href)
                if urlparse(full_url).netloc == base_netloc:
                    contact_urls.append(full_url)

        return list(dict.fromkeys(contact_urls))[:5]

    def _html_to_text(self, html: str) -> str:
        """–ë—ã—Å—Ç—Ä–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ HTML –≤ —Ç–µ–∫—Å—Ç (–±–µ–∑ Playwright)."""
        text = re.sub(r'<script[^>]*>.*?</script>', '', html, flags=re.DOTALL | re.IGNORECASE)
        text = re.sub(r'<style[^>]*>.*?</style>', '', text, flags=re.DOTALL | re.IGNORECASE)
        text = re.sub(r'<[^>]+>', ' ', text)
        text = re.sub(r'\s+', ' ', text).strip()
        return text

    async def _http_probe_page(self, client: httpx.AsyncClient, url: str) -> Dict:
        """HTTP GET –æ–¥–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã, –∏–∑–≤–ª–µ—á—å –ò–ù–ù/email –±–µ–∑ –±—Ä–∞—É–∑–µ—Ä–∞."""
        page_result: Dict = {
            "url": url, "inn": None, "emails": [], "error": None, "strategy": "http",
        }
        try:
            resp = await client.get(url)
            if resp.status_code not in (200, 301, 302):
                page_result["error"] = f"HTTP {resp.status_code}"
                return page_result
            html = resp.text
            actual_url = str(resp.url)
            page_result["url"] = actual_url

            text = self._html_to_text(html)
            inn = self.extract_inn(text, html)
            emails = self.extract_emails(text)
            emails.extend(self.extract_emails_from_html(html))
            emails = list(set(emails))

            if inn:
                page_result["inn"] = inn
            if emails:
                page_result["emails"] = emails

            # Tier 2: embedded data sniff
            if not inn:
                sniffed = self._sniff_embedded_data(html)
                if sniffed["inn"]:
                    page_result["inn"] = sniffed["inn"]
                    page_result["strategy"] = f"api_sniff:{sniffed['source']}"
                if sniffed["emails"] and not page_result["emails"]:
                    page_result["emails"] = sniffed["emails"]

            return page_result
        except Exception as e:
            page_result["error"] = str(e)[:200]
            return page_result

    async def _http_probe_domain(self, domain: str) -> Dict:
        """
        Tier 1+2: –ü–æ–ø—ã—Ç–∫–∞ –∏–∑–≤–ª–µ—á—å –ò–ù–ù/email —á–µ—Ä–µ–∑ HTTP (–±–µ–∑ Playwright).
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç. –ï—Å–ª–∏ inn –Ω–∞–π–¥–µ–Ω ‚Äî Playwright –Ω–µ –Ω—É–∂–µ–Ω.
        """
        t0 = time.monotonic()

        result: Dict = {
            'domain': domain,
            'inn': None,
            'emails': [],
            'source_urls': [],
            'error': None,
            'extraction_log': [],
            'strategy': 'http',
            'strategy_time_ms': 0,
            'js_required': False,
        }

        url = f"https://{domain}" if not domain.startswith('http') else domain
        base_url = url

        try:
            async with httpx.AsyncClient(
                timeout=httpx.Timeout(connect=5.0, read=8.0, write=5.0, pool=5.0),
                follow_redirects=True,
                headers=self._HTTP_HEADERS,
                verify=False,
            ) as client:
                # --- Main page ---
                main = await self._http_probe_page(client, url)
                result['source_urls'].append(main["url"])
                result['extraction_log'].append({
                    "url": main["url"],
                    "inn_found": main["inn"],
                    "emails_found": main["emails"],
                    "strategy": main["strategy"],
                })

                if main.get("error") and "403" in str(main["error"]):
                    result['js_required'] = True

                if main["inn"]:
                    result['inn'] = main["inn"]
                    result['strategy'] = main["strategy"]
                if main["emails"]:
                    result['emails'] = list(set(main["emails"]))

                # Early exit if INN found
                if result['inn'] and result['emails']:
                    result['strategy_time_ms'] = int((time.monotonic() - t0) * 1000)
                    logger.info(
                        f"  ‚ö° HTTP Tier1+2: –ò–ù–ù+Email –Ω–∞–π–¥–µ–Ω—ã –±–µ–∑ Playwright –∑–∞ "
                        f"{result['strategy_time_ms']}ms"
                    )
                    return result

                # --- Contact pages via HTTP ---
                # Get main page HTML for link extraction
                try:
                    main_resp = await client.get(url)
                    main_html = main_resp.text if main_resp.status_code == 200 else ""
                except Exception:
                    main_html = ""

                contact_urls = self._extract_contact_links_from_html(main_html, base_url)

                # Add common paths (including nested paths like /about/requisites/)
                common_paths = [
                    '/contacts', '/about', '/requisites', '/company',
                    '/kontakty', '/o-kompanii', '/rekvizity', '/legal',
                    '/info', '/rekvizity/', '/contacts/', '/about/',
                    '/about/requisites', '/about/requisites/', '/about/contacts',
                    '/about/contacts/', '/pages/requisites', '/about/company',
                ]
                for path in common_paths:
                    test_url = urljoin(base_url, path)
                    if test_url not in contact_urls:
                        contact_urls.append(test_url)

                for contact_url in contact_urls[:15]:
                    if result['inn'] and result['emails']:
                        break
                    cp = await self._http_probe_page(client, contact_url)
                    if cp.get("error"):
                        continue
                    result['source_urls'].append(cp["url"])
                    result['extraction_log'].append({
                        "url": cp["url"],
                        "inn_found": cp["inn"],
                        "emails_found": cp["emails"],
                        "strategy": cp["strategy"],
                    })
                    if cp["inn"] and not result['inn']:
                        result['inn'] = cp["inn"]
                        result['strategy'] = cp["strategy"]
                    if cp["emails"] and not result['emails']:
                        result['emails'] = list(set(cp["emails"]))

        except Exception as e:
            result['error'] = str(e)[:200]
            logger.warning(f"  ‚ö†Ô∏è HTTP probe failed for {domain}: {e}")

        result['strategy_time_ms'] = int((time.monotonic() - t0) * 1000)

        if result['inn']:
            logger.info(
                f"  ‚ö° HTTP: –ò–ù–ù –Ω–∞–π–¥–µ–Ω –±–µ–∑ Playwright –∑–∞ {result['strategy_time_ms']}ms"
            )

        return result

    # ‚îÄ‚îÄ‚îÄ Main entry point (multi-strategy) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    async def parse_domain(self, domain: str) -> Dict:
        """
        –ü–∞—Ä—Å–∏—Ç—å –¥–æ–º–µ–Ω –∏ –∏–∑–≤–ª–µ—á—å –ò–ù–ù –∏ email.
        Multi-strategy: HTTP first ‚Üí API sniff ‚Üí Playwright fallback.

        Args:
            domain: –î–æ–º–µ–Ω–Ω–æ–µ –∏–º—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, example.com)

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏: {domain, inn, emails, source_urls, error,
                                     extraction_log, strategy_used, strategy_time_ms}
        """
        t0 = time.monotonic()

        # === TIER 1+2: HTTP probe (no browser) ===
        http_result = await self._http_probe_domain(domain)

        # If HTTP found INN (primary target) ‚Üí skip Playwright entirely
        if http_result['inn']:
            http_result['emails'] = list(set(http_result.get('emails', [])))
            # Save strategy result for future domain override
            try:
                self.learning_engine.save_strategy_result(
                    domain, http_result.get('strategy', 'http'),
                    found_inn=True, found_email=bool(http_result['emails']),
                    time_ms=http_result.get('strategy_time_ms', 0),
                )
            except Exception:
                pass
            logger.info(
                f"‚úÖ {domain}: HTTP-only ‚Äî –ò–ù–ù={http_result['inn']}, "
                f"Email={http_result['emails']} [{http_result['strategy_time_ms']}ms]"
            )
            return {
                'domain': domain,
                'inn': http_result['inn'],
                'emails': http_result.get('emails', []),
                'source_urls': http_result.get('source_urls', []),
                'error': None,
                'extraction_log': http_result.get('extraction_log', []),
                'strategy_used': http_result.get('strategy', 'http'),
                'strategy_time_ms': http_result.get('strategy_time_ms', 0),
            }

        # Keep HTTP-found data for merging with Playwright results
        http_emails = list(set(http_result.get('emails', [])))
        http_source_urls = http_result.get('source_urls', [])
        http_extraction_log = http_result.get('extraction_log', [])

        # === TIER 3: Playwright (browser-based, expensive) ===
        logger.info(f"  üåê HTTP –Ω–µ –Ω–∞—à—ë–ª –ò–ù–ù –¥–ª—è {domain}, –ø–µ—Ä–µ—Ö–æ–¥–∏–º –∫ Playwright...")
        pw_result = await self._playwright_parse_domain(domain)

        # Merge HTTP findings with Playwright results
        pw_result['emails'] = list(set(
            pw_result.get('emails', []) + http_emails
        ))
        pw_result['source_urls'] = list(dict.fromkeys(
            http_source_urls + pw_result.get('source_urls', [])
        ))
        pw_result['extraction_log'] = (
            http_extraction_log + pw_result.get('extraction_log', [])
        )
        pw_result['strategy_used'] = 'playwright'
        pw_result['strategy_time_ms'] = int((time.monotonic() - t0) * 1000)

        # Save strategy result for future domain override
        try:
            self.learning_engine.save_strategy_result(
                domain, 'playwright',
                found_inn=bool(pw_result.get('inn')),
                found_email=bool(pw_result.get('emails')),
                time_ms=pw_result.get('strategy_time_ms', 0),
            )
        except Exception:
            pass

        if pw_result.get('inn'):
            logger.info(
                f"‚úÖ {domain}: Playwright ‚Äî –ò–ù–ù={pw_result['inn']}, "
                f"Email={pw_result['emails']} [{pw_result['strategy_time_ms']}ms]"
            )
        else:
            logger.info(
                f"‚ö†Ô∏è {domain}: –ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ (HTTP+Playwright) "
                f"[{pw_result['strategy_time_ms']}ms]"
            )

        return pw_result

    # ‚îÄ‚îÄ‚îÄ Tier 3: Playwright-based parsing ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    async def _playwright_parse_domain(self, domain: str) -> Dict:
        """
        Playwright-based domain parsing (Tier 3).
        –í—ã–∑—ã–≤–∞–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ HTTP probe –Ω–µ –Ω–∞—à—ë–ª –ò–ù–ù.
        """
        if not self.browser:
            raise Exception("–ë—Ä–∞—É–∑–µ—Ä –Ω–µ –∑–∞–ø—É—â–µ–Ω. –í—ã–∑–æ–≤–∏—Ç–µ start() —Å–Ω–∞—á–∞–ª–∞.")
        
        result = {
            'domain': domain,
            'inn': None,
            'emails': [],
            'source_urls': [],
            'error': None,
            'extraction_log': [],
        }
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º URL
        url = f"https://{domain}" if not domain.startswith('http') else domain
        base_url = url
        
        logger.info(f"üîç –ü–∞—Ä—Å–∏–Ω–≥: {domain}")
        
        page = await self.browser.new_page()
        
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
            logger.info(f"  ‚Üí –ó–∞–≥—Ä—É–∑–∫–∞ –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã...")
            await self.goto_with_fallback(page, url)
            result['source_urls'].append(page.url)
            
            # –ñ–¥–µ–º –Ω–µ–º–Ω–æ–≥–æ –¥–ª—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            await page.wait_for_timeout(500)
            
            # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç –∏ HTML –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            main_text = await self.get_page_text(page)
            main_html = await page.content()
            
            # –ò—â–µ–º –ò–ù–ù –∏ email –Ω–∞ –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ
            inn = self.extract_inn(main_text, main_html)
            emails = self.extract_emails(main_text)
            emails.extend(self.extract_emails_from_html(main_html))
            
            page_log = {"url": page.url, "inn_found": None, "emails_found": []}
            if inn:
                result['inn'] = inn
                page_log["inn_found"] = inn
                logger.info(f"  ‚úÖ –ò–ù–ù –Ω–∞–π–¥–µ–Ω –Ω–∞ –≥–ª–∞–≤–Ω–æ–π: {inn}")
            else:
                page_log["inn_found"] = None
            
            if emails:
                result['emails'].extend(emails)
                page_log["emails_found"] = list(set(emails))
                logger.info(f"  ‚úÖ Email –Ω–∞–π–¥–µ–Ω –Ω–∞ –≥–ª–∞–≤–Ω–æ–π: {emails}")
            
            result['extraction_log'].append(page_log)
            
            # –ï—Å–ª–∏ –ò–ù–ù –∏ email —É–∂–µ –Ω–∞–π–¥–µ–Ω—ã –Ω–∞ –≥–ª–∞–≤–Ω–æ–π ‚Äî —Ä–∞–Ω–Ω–∏–π –≤—ã—Ö–æ–¥
            if inn and emails:
                logger.info(f"  ‚ö° –ò–ù–ù + email –Ω–∞–π–¥–µ–Ω—ã –Ω–∞ –≥–ª–∞–≤–Ω–æ–π ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –¥–æ–ø. —Å—Ç—Ä–∞–Ω–∏—Ü—ã")
            else:
                # –ò—â–µ–º –Ω–∞ –∫–æ–Ω—Ç–∞–∫—Ç–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü–∞—Ö
                logger.info(f"  ‚Üí –ü–æ–∏—Å–∫ –∫–æ–Ω—Ç–∞–∫—Ç–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü...")
                contact_urls = await self.find_contact_pages(page, base_url)
                priority_urls = self._build_priority_urls(domain, base_url)
                if priority_urls:
                    logger.info(f"  üéì –ù–∞–π–¥–µ–Ω–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã—Ö URL –∏–∑ –æ–±—É—á–µ–Ω–∏—è: {len(priority_urls)}")
                    contact_urls = priority_urls + [url for url in contact_urls if url not in priority_urls]

                # –ü—Ä–æ–±—É–µ–º –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ URL (—Å–æ–∫—Ä–∞—â—ë–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Å–∞–º—ã—Ö —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö)
                common_paths = [
                    '/contacts', '/contacts/', '/about', '/about/',
                    '/requisites', '/requisites/', '/company', '/company/',
                    '/kontakty', '/kontakty/', '/o-kompanii', '/o-kompanii/',
                    '/legal', '/legal/', '/pages/requisites/', '/info', '/info/',
                    '/requisiti', '/requisiti.html', '/rekvizity', '/rekvizity/',
                    '/ur-lica', '/ur-licam', '/opt', '/opt.html',
                    '/kontakty.html', '/o-kompanii.html', '/about-us',
                    '/contacts.html', '/company.html',
                    '/about/requisites', '/about/requisites/', '/about/contacts',
                    '/about/contacts/', '/about/company', '/about/company/',
                    '/pages/contacts', '/pages/about',
                ]
                for path in common_paths:
                    if inn and emails:
                        break  # –£–∂–µ –Ω–∞—à–ª–∏ –≤—Å—ë –Ω—É–∂–Ω–æ–µ
                    test_url = urljoin(base_url, path)
                    if test_url in contact_urls:
                        continue
                    try:
                        response = await page.goto(test_url, wait_until='domcontentloaded', timeout=7000)
                        if response and response.ok:
                            contact_urls.append(page.url)
                    except Exception:
                        pass

                contact_urls = list(dict.fromkeys(contact_urls))

                # Also collect PDF links from the main page for INN extraction
                pdf_urls = []
                try:
                    pdf_links = await page.evaluate('''() => {
                        return Array.from(document.querySelectorAll('a[href]'))
                            .filter(a => a.href.toLowerCase().endsWith('.pdf'))
                            .map(a => ({ href: a.href, text: a.innerText.toLowerCase() }));
                    }''')
                    pdf_keywords = ['—Ä–µ–∫–≤–∏–∑–∏—Ç', 'requisit', '–∫–∞—Ä—Ç–æ—á–∫', 'card', '–∏–Ω–Ω', 'inn', '–∫–æ–º–ø–∞–Ω–∏', 'company', '—é—Ä–∏–¥–∏—á–µ—Å–∫', 'legal']
                    for link in pdf_links:
                        href = link['href']
                        text = link.get('text', '')
                        if any(kw in text for kw in pdf_keywords) or any(kw in href.lower() for kw in pdf_keywords):
                            full_url = urljoin(base_url, href)
                            if full_url not in pdf_urls and full_url not in contact_urls:
                                pdf_urls.append(full_url)
                except Exception:
                    pass

                for contact_url in contact_urls:
                    if inn and emails:
                        break  # –†–∞–Ω–Ω–∏–π –≤—ã—Ö–æ–¥ ‚Äî –≤—Å—ë –Ω–∞–π–¥–µ–Ω–æ
                    try:
                        # Handle PDF URLs separately
                        if self._is_pdf_url(contact_url):
                            logger.info(f"  ‚Üí PDF: {contact_url}")
                            result['source_urls'].append(contact_url)
                            pdf_result = await self.download_and_parse_pdf(contact_url)
                            cp_log = {"url": contact_url, "inn_found": None, "emails_found": [], "pdf": True}
                            if pdf_result.get("error"):
                                cp_log["error"] = pdf_result["error"]
                            if pdf_result.get("inn") and not inn:
                                inn = pdf_result["inn"]
                                result['inn'] = inn
                                cp_log["inn_found"] = inn
                            if pdf_result.get("emails"):
                                new_emails = pdf_result["emails"]
                                if not emails:
                                    emails = new_emails
                                result['emails'].extend(new_emails)
                                cp_log["emails_found"] = list(set(new_emails))
                            result['extraction_log'].append(cp_log)
                            continue

                        logger.info(f"  ‚Üí –ó–∞–≥—Ä—É–∑–∫–∞: {contact_url}")
                        await page.goto(contact_url, wait_until='domcontentloaded', timeout=self.timeout)
                        result['source_urls'].append(page.url)

                        contact_text = await self.get_page_text(page)
                        contact_html = await page.content()

                        cp_log = {"url": page.url, "inn_found": None, "emails_found": []}

                        contact_inn = self.extract_inn(contact_text, contact_html)
                        if contact_inn:
                            inn = contact_inn
                            result['inn'] = inn
                            cp_log["inn_found"] = inn
                            logger.info(f"  ‚úÖ –ò–ù–ù –Ω–∞–π–¥–µ–Ω –Ω–∞ –∫–æ–Ω—Ç–∞–∫—Ç–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ: {inn}")

                        if not emails:
                            new_emails = self.extract_emails(contact_text)
                            new_emails.extend(self.extract_emails_from_html(contact_html))
                            if new_emails:
                                emails = new_emails
                                result['emails'].extend(new_emails)
                                cp_log["emails_found"] = list(set(new_emails))
                                logger.info(f"  ‚úÖ Email –Ω–∞–π–¥–µ–Ω –Ω–∞ –∫–æ–Ω—Ç–∞–∫—Ç–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ: {new_emails}")

                        # Collect PDF links from this page too
                        if not inn:
                            try:
                                page_pdf_links = await page.evaluate('''() => {
                                    return Array.from(document.querySelectorAll('a[href]'))
                                        .filter(a => a.href.toLowerCase().endsWith('.pdf'))
                                        .map(a => a.href);
                                }''')
                                for pdf_href in page_pdf_links:
                                    full_pdf = urljoin(base_url, pdf_href)
                                    if full_pdf not in pdf_urls and urlparse(full_pdf).netloc == urlparse(base_url).netloc:
                                        pdf_urls.append(full_pdf)
                            except Exception:
                                pass

                        result['extraction_log'].append(cp_log)

                    except PlaywrightTimeout:
                        result['extraction_log'].append({"url": contact_url, "error": "timeout"})
                        logger.warning(f"  ‚è±Ô∏è –¢–∞–π–º–∞—É—Ç –∑–∞–≥—Ä—É–∑–∫–∏: {contact_url}")
                    except Exception as e:
                        err_str = str(e)
                        # If download started (PDF link), try PDF parsing
                        if 'download' in err_str.lower():
                            logger.info(f"  üìÑ –û–±–Ω–∞—Ä—É–∂–µ–Ω PDF (download): {contact_url}")
                            pdf_result = await self.download_and_parse_pdf(contact_url)
                            cp_log = {"url": contact_url, "inn_found": None, "emails_found": [], "pdf": True}
                            if pdf_result.get("error"):
                                cp_log["error"] = pdf_result["error"]
                            if pdf_result.get("inn") and not inn:
                                inn = pdf_result["inn"]
                                result['inn'] = inn
                                cp_log["inn_found"] = inn
                            if pdf_result.get("emails"):
                                new_emails = pdf_result["emails"]
                                if not emails:
                                    emails = new_emails
                                result['emails'].extend(new_emails)
                                cp_log["emails_found"] = list(set(new_emails))
                            result['extraction_log'].append(cp_log)
                        else:
                            result['extraction_log'].append({"url": contact_url, "error": err_str[:200]})
                            logger.warning(f"  ‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {contact_url}: {e}")

                # Parse collected PDF links (if INN still not found)
                for pdf_url in pdf_urls:
                    if inn:
                        break
                    try:
                        logger.info(f"  ‚Üí PDF: {pdf_url}")
                        result['source_urls'].append(pdf_url)
                        pdf_result = await self.download_and_parse_pdf(pdf_url)
                        cp_log = {"url": pdf_url, "inn_found": None, "emails_found": [], "pdf": True}
                        if pdf_result.get("error"):
                            cp_log["error"] = pdf_result["error"]
                        if pdf_result.get("inn"):
                            inn = pdf_result["inn"]
                            result['inn'] = inn
                            cp_log["inn_found"] = inn
                        if pdf_result.get("emails"):
                            new_emails = pdf_result["emails"]
                            if not emails:
                                emails = new_emails
                            result['emails'].extend(new_emails)
                            cp_log["emails_found"] = list(set(new_emails))
                        result['extraction_log'].append(cp_log)
                    except Exception as e:
                        result['extraction_log'].append({"url": pdf_url, "error": str(e)[:200]})
                        logger.warning(f"  ‚ö†Ô∏è –û—à–∏–±–∫–∞ PDF {pdf_url}: {e}")
            
            # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã email
            result['emails'] = list(set(result['emails']))
            
            if result['inn'] or result['emails']:
                logger.info(f"‚úÖ {domain}: –ò–ù–ù={result['inn']}, Email={result['emails']}")
            else:
                logger.warning(f"‚ö†Ô∏è {domain}: –ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
            
        except PlaywrightTimeout:
            error_msg = f"–¢–∞–π–º–∞—É—Ç –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"
            result['error'] = error_msg
            result['extraction_log'].append({"url": url, "error": "timeout"})
            logger.error(f"‚ùå {domain}: {error_msg}")
            
        except Exception as e:
            error_msg = f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {str(e)}"
            result['error'] = error_msg
            result['extraction_log'].append({"url": url, "error": str(e)[:200]})
            logger.error(f"‚ùå {domain}: {error_msg}")
            
        finally:
            await page.close()
        
        return result
    
    async def parse_domains(self, domains: List[str]) -> List[Dict]:
        """
        –ü–∞—Ä—Å–∏—Ç—å —Å–ø–∏—Å–æ–∫ –¥–æ–º–µ–Ω–æ–≤.
        
        Args:
            domains: –°–ø–∏—Å–æ–∫ –¥–æ–º–µ–Ω–æ–≤
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–æ–º–µ–Ω–∞
        """
        results = []
        
        for i, domain in enumerate(domains, 1):
            logger.info(f"\n{'='*60}")
            logger.info(f"–î–æ–º–µ–Ω {i}/{len(domains)}")
            logger.info(f"{'='*60}")
            
            result = await self.parse_domain(domain)
            results.append(result)
            
            # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
            await asyncio.sleep(0.2)
        
        return results
